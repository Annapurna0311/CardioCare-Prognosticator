# -*- coding: utf-8 -*-
"""Heart Disease Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RCg5X-cJz7eV5wrZpWTKSNbb4CLr74vR

Importing the Dependencies
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

"""Data Collection and Processing"""

# loading the csv data to a Pandas DataFrame
heart_data = pd.read_csv('/content/data.csv')

# print first 5 rows of the dataset
heart_data.head()

# print last 5 rows of the dataset
heart_data.tail()

# number of rows and columns in the dataset
heart_data.shape

# getting some info about the data
heart_data.info()

# checking for missing values
heart_data.isnull().sum()

# statistical measures about the data
heart_data.describe()

heart_data['sex']=heart_data['sex'].replace(['Male','Female'],[1,2])

# checking the distribution of Target Variable
heart_data['chol'].value_counts()

"""1 --> Defective Heart

0 --> Healthy Heart

Splitting the Features and Target
"""

X = heart_data.drop(columns='chol', axis=1)
Y = heart_data['chol']

print(X)

print(Y)

"""Splitting the Data into Training data & Test Data"""

from sklearn.model_selection import train_test_split
import numpy as np

# Assuming X and Y are your feature and target variables

# Remove rows with NaN values from both X and Y
mask = ~np.isnan(Y)
X = X[mask]
Y = Y[mask]

# Perform train-test split without stratification
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

"""Model Training

Logistic Regression
"""

import tensorflow as tf
from tensorflow.keras import layers, models

model=models.Sequential()

heart_data['sex']=heart_data['sex'].replace(['Male','Female'],[1,2])

from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

# Assuming X and Y are your feature and target variables

# Remove rows with NaN values from both X and Y
mask = ~np.isnan(Y)
X = X[mask]
Y = Y[mask]

# Identify categorical columns (replace 'categorical_column_names' with your actual categorical column names)
categorical_column_names = ['sex', 'cp']

# Identify numerical columns (replace 'numerical_column_names' with your actual numerical column names)
numerical_column_names = ['oldpeak', 'num']

# Create transformers for numerical and categorical columns
numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean'))])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine transformers using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_column_names),
        ('cat', categorical_transformer, categorical_column_names)
    ])

# Apply preprocessing and train-test split
X_processed = preprocessor.fit_transform(X)

X_train, X_test, Y_train, Y_test = train_test_split(X_processed, Y, test_size=0.2, random_state=2)

# Instantiate and fit your model
model = RandomForestClassifier()  # You can replace this with your preferred classifier
model.fit(X_train, Y_train)

"""Model Evaluation

Accuracy Score
"""

# accuracy on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy on Training data : ', training_data_accuracy)

# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy on Test data : ', test_data_accuracy)

from sklearn.model_selection import cross_val_score

# Replace 'model' with your instantiated model
cv_scores = cross_val_score(model, X_processed, Y, cv=5, scoring='accuracy')
print(f'Cross-validated Accuracy: {np.mean(cv_scores):.2f} +/- {np.std(cv_scores):.2f}')

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Replace 'model' with RandomForestClassifier or another algorithm you want to try
model = RandomForestClassifier(random_state=42)

# Cross-validated accuracy
cv_scores = cross_val_score(model, X_processed, Y, cv=5, scoring='accuracy')
print(f'Cross-validated Accuracy: {np.mean(cv_scores):.2f} +/- {np.std(cv_scores):.2f}')

"""Building a Predictive System"""

input_data = (62,20,10,140,268,78,160,40,36,45,78,65,96,37,21,85)

# change the input data to a numpy array
input_data_as_numpy_array= np.asarray(input_data)

# reshape the numpy array as we are predicting for only on instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = model.predict(input_data_reshaped)
print(prediction)

if (prediction[0]== 0):
  print('The Person does not have a Heart Disease')
else:
  print('The Person has Heart Disease')
